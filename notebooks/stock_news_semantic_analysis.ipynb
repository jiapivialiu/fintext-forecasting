{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7089b945d50d4c8ab2bbaded97f1ca98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab0d59b4225d44d1887fe23b69f67912",
              "IPY_MODEL_640381bb940b474e8ee2b9d12e2e2eef",
              "IPY_MODEL_26d5b69b79b14e1fac39cb8562750f49"
            ],
            "layout": "IPY_MODEL_66fe9607ab8842d0b96a62612cfbe450"
          }
        },
        "ab0d59b4225d44d1887fe23b69f67912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb66125b3e424fabb6b7ec9d67d5bb5f",
            "placeholder": "​",
            "style": "IPY_MODEL_43d3007c0f11446e87eba71d395c60f9",
            "value": "Map: 100%"
          }
        },
        "640381bb940b474e8ee2b9d12e2e2eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbf54e0922de40ed82751ebcb4331771",
            "max": 20781,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64213c7a4df64c2ca664ef00263a4b8a",
            "value": 20781
          }
        },
        "26d5b69b79b14e1fac39cb8562750f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2faf066ca3d74348b89d496efb108934",
            "placeholder": "​",
            "style": "IPY_MODEL_ec9b5c60740b4dd3b16db0bc9430f5f8",
            "value": " 20781/20781 [01:38&lt;00:00, 194.79 examples/s]"
          }
        },
        "66fe9607ab8842d0b96a62612cfbe450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb66125b3e424fabb6b7ec9d67d5bb5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43d3007c0f11446e87eba71d395c60f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbf54e0922de40ed82751ebcb4331771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64213c7a4df64c2ca664ef00263a4b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2faf066ca3d74348b89d496efb108934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec9b5c60740b4dd3b16db0bc9430f5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8ae16d489394b07be41d56e96646370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1364bda5a8c40ff9c48ff1c65b6856a",
              "IPY_MODEL_8308fa4d767a4042a434ac547f1ee51b",
              "IPY_MODEL_2a28d70f611747278b045d4aba77261d"
            ],
            "layout": "IPY_MODEL_0b300108d9da4ea8b73d16ba28201621"
          }
        },
        "f1364bda5a8c40ff9c48ff1c65b6856a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27821ffe452f461ba2abfde6fa322254",
            "placeholder": "​",
            "style": "IPY_MODEL_7b33d2ab9fe4489c9167df50a9c246bc",
            "value": "Map: 100%"
          }
        },
        "8308fa4d767a4042a434ac547f1ee51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0704f1163773466daba7f077fc004fd4",
            "max": 2555,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ccdcea722404c7f91759173d39e2055",
            "value": 2555
          }
        },
        "2a28d70f611747278b045d4aba77261d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d3b88db45464c599511c9a13e7007ea",
            "placeholder": "​",
            "style": "IPY_MODEL_835dd1d7860943b1b681039d18073cff",
            "value": " 2555/2555 [00:12&lt;00:00, 202.13 examples/s]"
          }
        },
        "0b300108d9da4ea8b73d16ba28201621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27821ffe452f461ba2abfde6fa322254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b33d2ab9fe4489c9167df50a9c246bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0704f1163773466daba7f077fc004fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ccdcea722404c7f91759173d39e2055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d3b88db45464c599511c9a13e7007ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835dd1d7860943b1b681039d18073cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Nets on Financial Time Seris Forecasting on Text with JAX"
      ],
      "metadata": {
        "id": "4DA2tTnlQjZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RQREEVdWQzco"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acl_train = load_dataset(\"TheFinAI/flare-sm-acl\", split=\"train\")\n",
        "acl_test = load_dataset(\"TheFinAI/flare-sm-acl\", split=\"test\")\n",
        "acl_valid = load_dataset(\"TheFinAI/flare-sm-acl\", split=\"valid\")\n",
        "\n",
        "acl_train_df = acl_train.to_pandas()[['gold', 'text']]\n",
        "acl_valid_df = acl_valid.to_pandas()[['gold', 'text']]\n",
        "acl_test_df = acl_test.to_pandas()[['gold', 'text']]"
      ],
      "metadata": {
        "id": "YLLSDoAMbE0I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "mu9tDJuu-qDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2') # load pre-trained model for text embedding\n",
        "\n",
        "# Text embedding with batch processing\n",
        "def batch_encode(texts, batch_size=32):\n",
        "    def get_sbert_embeddings(texts):\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "        return embeddings\n",
        "    embeddings_list = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        batch_embeddings = get_sbert_embeddings(batch)\n",
        "        embeddings_list.append(batch_embeddings)\n",
        "    return np.vstack(embeddings_list)"
      ],
      "metadata": {
        "id": "NoEM5P4lQwxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# training data processing\n",
        "texts = acl_train_df['text'].tolist()\n",
        "X_train_embeddings = batch_encode(texts)\n",
        "X_train_embeddings = np.array(X_train_embeddings, dtype=np.float32)\n",
        "acl_train_embedded = np.concatenate([acl_train_df[['gold']], X_train_embeddings], axis=1)\n",
        "acl_train_embedded_jax = jnp.array(acl_train_embedded, dtype=jnp.float32)"
      ],
      "metadata": {
        "id": "85cP-6q-Q9x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation data processing\n",
        "valid_texts = acl_valid_df['text'].tolist()\n",
        "X_valid_embeddings = batch_encode(valid_texts)\n",
        "X_valid_embeddings = np.array(X_valid_embeddings, dtype=np.float32)\n",
        "acl_valid_embedded = np.concatenate([acl_valid_df[['gold']], X_valid_embeddings], axis=1)\n",
        "acl_valid_embedded_jax = jnp.array(acl_valid_embedded, dtype=jnp.float32)\n",
        "\n",
        "# test data processing\n",
        "test_texts = acl_test_df['text'].tolist()\n",
        "X_test_embeddings = batch_encode(test_texts)\n",
        "X_test_embeddings = np.array(X_test_embeddings, dtype=np.float32)\n",
        "acl_test_embedded = np.concatenate([acl_test_df[['gold']], X_test_embeddings], axis=1)\n",
        "acl_test_embedded_jax = jnp.array(acl_test_embedded, dtype=jnp.float32)"
      ],
      "metadata": {
        "id": "aTPnCGTzSl8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acl_train_embedded[:3])\n",
        "print(acl_train_embedded_jax[:3])\n",
        "print(acl_train_embedded.shape)\n",
        "print(acl_valid_embedded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gNW-7F4-jBh",
        "outputId": "09729c64-f2f9-4b1d-b7a3-9a133442993e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.         -0.00104391 -0.0153452  ... -0.06044361 -0.04032699\n",
            "   0.04581903]\n",
            " [ 1.         -0.01484502 -0.01614808 ... -0.05998765 -0.04143057\n",
            "   0.04037762]\n",
            " [ 0.         -0.00950441 -0.01646534 ... -0.06522585 -0.03886713\n",
            "   0.03963047]]\n",
            "[[ 1.         -0.00104391 -0.0153452  ... -0.06044361 -0.04032699\n",
            "   0.04581903]\n",
            " [ 1.         -0.01484502 -0.01614808 ... -0.05998765 -0.04143057\n",
            "   0.04037762]\n",
            " [ 0.         -0.00950441 -0.01646534 ... -0.06522585 -0.03886713\n",
            "   0.03963047]]\n",
            "(20781, 385)\n",
            "(2555, 385)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the models"
      ],
      "metadata": {
        "id": "mv48-cZA-uuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Logistic regression"
      ],
      "metadata": {
        "id": "Z2ViiZbQS2XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for training. Split training data into X and response Y."
      ],
      "metadata": {
        "id": "RyN3ksOb-yu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = acl_train_embedded[:, 0].reshape(-1, 1)\n",
        "X_train = acl_train_embedded[:, 1:]\n",
        "y_valid = acl_valid_embedded[:, 0].reshape(-1, 1)\n",
        "X_valid = acl_valid_embedded[:, 1:]"
      ],
      "metadata": {
        "id": "yHcxwjG94DOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions"
      ],
      "metadata": {
        "id": "jp1DrbZG-6aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train logistic regression model\n",
        "maxiter = 30\n",
        "lr_model = LogisticRegression(max_iter=maxiter, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_pred = lr_model.predict(X_valid)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"\\nLogistic Regression Results:\")\n",
        "print(f\"Validation Accuracy: {lr_model.score(X_valid, y_valid):.10f}\")"
      ],
      "metadata": {
        "id": "LSogRwgPR2ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f85f920-6b21-4b87-fa3b-da9ab20b96f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Results:\n",
            "Validation Accuracy: 0.4845401174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Train a MLP with JAX"
      ],
      "metadata": {
        "id": "dnjmiWjNS9ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from typing import List, Tuple, Any\n",
        "import optax  # For Adam optimizer\n",
        "from functools import partial\n",
        "\n",
        "# Prepare data for modelling\n",
        "y_train = acl_train_embedded_jax[:, 0].reshape(-1, 1)\n",
        "X_train = acl_train_embedded_jax[:, 1:]\n",
        "y_valid = acl_valid_embedded_jax[:, 0].reshape(-1, 1)\n",
        "X_valid = acl_valid_embedded_jax[:, 1:]\n",
        "input_dim = X_train.shape[1]  # Get input dimension from training data"
      ],
      "metadata": {
        "id": "WX511UFt-77w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MLP in JAX\n",
        "def init_mlp_params(layer_sizes: List[int], key: Any) -> List[Tuple[jnp.ndarray, jnp.ndarray]]:\n",
        "    params = []\n",
        "    keys = random.split(key, len(layer_sizes))\n",
        "    for in_dim, out_dim, k in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
        "        w_key, b_key = random.split(k)\n",
        "        W = random.normal(w_key, (in_dim, out_dim)) * jnp.sqrt(2. / in_dim)\n",
        "        b = jnp.zeros((out_dim,))\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def mlp_forward(params: List[Tuple[jnp.ndarray, jnp.ndarray]], x: jnp.ndarray, dropout_rate: float = 0.0,\n",
        "               train: bool = False, key: Any = None) -> jnp.ndarray:\n",
        "    \"\"\"Forward pass with dropout support\"\"\"\n",
        "    for i, (W, b) in enumerate(params[:-1]):\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        if train and dropout_rate > 0:\n",
        "            if key is None:\n",
        "                raise ValueError(\"Random key required for dropout\")\n",
        "            dropout_key = random.fold_in(key, i)  # Different key for each layer\n",
        "            mask = random.bernoulli(dropout_key, p=1-dropout_rate, shape=x.shape)\n",
        "            x = x * mask / (1 - dropout_rate)  # Scale to maintain expected value\n",
        "\n",
        "    W_last, b_last = params[-1]\n",
        "    logits = jnp.dot(x, W_last) + b_last\n",
        "    return logits\n",
        "\n",
        "# Loss and training step\n",
        "def binary_cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
        "    preds = jax.nn.sigmoid(logits)\n",
        "    return -jnp.mean(labels * jnp.log(preds + 1e-7) + (1 - labels) * jnp.log(1 - preds + 1e-7))\n",
        "\n",
        "# Improved training step with Adam optimizer\n",
        "@partial(jax.jit, static_argnums=(4, 5))\n",
        "def train_step(params, X_batch, y_batch, opt_state, dropout_rate=0.2, train=True):\n",
        "    \"\"\"Single training step with Adam optimizer and dropout\"\"\"\n",
        "    key = random.PRNGKey(0)  # For reproducibility\n",
        "\n",
        "    def loss_fn(p):\n",
        "        logits = mlp_forward(p, X_batch, dropout_rate=dropout_rate, train=train, key=key)\n",
        "        return binary_cross_entropy_loss(logits, y_batch)\n",
        "\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, loss"
      ],
      "metadata": {
        "id": "HHNYQxYgR7o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "def tune_hyperparameters():\n",
        "    best_accuracy = 0.0\n",
        "    best_params = None\n",
        "    best_config = {}\n",
        "\n",
        "    # Define hyperparameter search space\n",
        "    learning_rates = [1e-4]\n",
        "    hidden_layer_configs = [\n",
        "        [128, 64]\n",
        "    ]\n",
        "    dropout_rates = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, .7, .8, .9]\n",
        "    batch_sizes = [32]  # For mini-batch training\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for hidden_layers in hidden_layer_configs:\n",
        "            for dropout_rate in dropout_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    print(f\"\\nTrying: lr={lr}, layers={hidden_layers}, dropout={dropout_rate}, batch_size={batch_size}\")\n",
        "\n",
        "                    # Initialize model\n",
        "                    key = random.PRNGKey(42)\n",
        "                    layer_sizes = [input_dim] + hidden_layers + [1]\n",
        "                    params = init_mlp_params(layer_sizes, key)\n",
        "\n",
        "                    # Initialize optimizer\n",
        "                    global optimizer  # Make it accessible in train_step\n",
        "                    optimizer = optax.adam(learning_rate=lr)\n",
        "                    opt_state = optimizer.init(params)\n",
        "\n",
        "                    # Mini-batch training\n",
        "                    num_batches = max(1, len(X_train) // batch_size)\n",
        "\n",
        "                    for epoch in range(50):  # Fewer epochs for tuning\n",
        "                        # Shuffle data\n",
        "                        perm = random.permutation(key, len(X_train))\n",
        "                        key = random.fold_in(key, epoch)  # Update key for next epoch\n",
        "\n",
        "                        # Mini-batch updates\n",
        "                        total_loss = 0.0\n",
        "                        for i in range(num_batches):\n",
        "                            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "                            X_batch = X_train[batch_idx]\n",
        "                            y_batch = y_train[batch_idx]\n",
        "\n",
        "                            params, opt_state, loss = train_step(\n",
        "                                params, X_batch, y_batch, opt_state,\n",
        "                                dropout_rate=dropout_rate, train=True\n",
        "                            )\n",
        "                            total_loss += loss\n",
        "\n",
        "                        avg_loss = total_loss / num_batches\n",
        "                        if epoch % 10 == 0:\n",
        "                            print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    # Evaluate on validation set\n",
        "                    val_accuracy = evaluate(params, X_valid, y_valid, dropout_rate=0.0, train=False)\n",
        "                    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "                    # Record result\n",
        "                    config = {\n",
        "                        'learning_rate': lr,\n",
        "                        'hidden_layers': hidden_layers,\n",
        "                        'dropout_rate': dropout_rate,\n",
        "                        'batch_size': batch_size,\n",
        "                        'val_accuracy': val_accuracy\n",
        "                    }\n",
        "                    results.append(config)\n",
        "\n",
        "                    # Update best model\n",
        "                    if val_accuracy > best_accuracy:\n",
        "                        best_accuracy = val_accuracy\n",
        "                        best_params = params\n",
        "                        best_config = config\n",
        "\n",
        "    print(\"\\n=== Hyperparameter Tuning Results ===\")\n",
        "    for i, res in enumerate(sorted(results, key=lambda x: x['val_accuracy'], reverse=True)):\n",
        "        print(f\"{i+1}. Accuracy: {res['val_accuracy']:.4f} - LR: {res['learning_rate']}, \"\n",
        "              f\"Layers: {res['hidden_layers']}, Dropout: {res['dropout_rate']}, \"\n",
        "              f\"Batch Size: {res['batch_size']}\")\n",
        "\n",
        "    print(f\"\\nBest Configuration: {best_config}\")\n",
        "    return best_params, best_config\n",
        "\n",
        "# Modified evaluation function for hyperparameter tuning\n",
        "def evaluate(params, X, y, dropout_rate=0.0, train=False) -> float:\n",
        "    \"\"\"Evaluate model accuracy with optional dropout\"\"\"\n",
        "    key = random.PRNGKey(99) if train else None\n",
        "    logits = mlp_forward(params, X, dropout_rate=dropout_rate, train=train, key=key)\n",
        "    preds = jax.nn.sigmoid(logits)\n",
        "    binary_preds = (preds > 0.5).astype(jnp.float32)\n",
        "    accuracy = jnp.mean(binary_preds == y)\n",
        "    return float(accuracy)"
      ],
      "metadata": {
        "id": "wg260zcbV7s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run hyperparameter tuning\n",
        "print(\"\\n=== Starting Hyperparameter Tuning ===\")\n",
        "best_params, best_config = tune_hyperparameters()\n",
        "\n",
        "# Train final model with best hyperparameters\n",
        "print(\"\\n=== Training Final Model with Best Hyperparameters ===\")\n",
        "key = random.PRNGKey(0)\n",
        "layer_sizes = [input_dim] + best_config['hidden_layers'] + [1]\n",
        "params = init_mlp_params(layer_sizes, key)\n",
        "\n",
        "# Initialize optimizer with best learning rate\n",
        "optimizer = optax.adam(learning_rate=best_config['learning_rate'])\n",
        "opt_state = optimizer.init(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVKPdWUfWVVl",
        "outputId": "b5bb940a-d1a7-4c26-dd41-c8af185cb608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting Hyperparameter Tuning ===\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.0, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6912\n",
            "Epoch 11 | Avg Loss: 0.6760\n",
            "Epoch 21 | Avg Loss: 0.6739\n",
            "Epoch 31 | Avg Loss: 0.6727\n",
            "Epoch 41 | Avg Loss: 0.6715\n",
            "Validation Accuracy: 0.4975\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.2, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6920\n",
            "Epoch 11 | Avg Loss: 0.6777\n",
            "Epoch 21 | Avg Loss: 0.6752\n",
            "Epoch 31 | Avg Loss: 0.6739\n",
            "Epoch 41 | Avg Loss: 0.6730\n",
            "Validation Accuracy: 0.5045\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.3, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6922\n",
            "Epoch 11 | Avg Loss: 0.6781\n",
            "Epoch 21 | Avg Loss: 0.6756\n",
            "Epoch 31 | Avg Loss: 0.6745\n",
            "Epoch 41 | Avg Loss: 0.6734\n",
            "Validation Accuracy: 0.5072\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.4, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6926\n",
            "Epoch 11 | Avg Loss: 0.6788\n",
            "Epoch 21 | Avg Loss: 0.6768\n",
            "Epoch 31 | Avg Loss: 0.6752\n",
            "Epoch 41 | Avg Loss: 0.6738\n",
            "Validation Accuracy: 0.5025\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.5, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6931\n",
            "Epoch 11 | Avg Loss: 0.6794\n",
            "Epoch 21 | Avg Loss: 0.6775\n",
            "Epoch 31 | Avg Loss: 0.6760\n",
            "Epoch 41 | Avg Loss: 0.6745\n",
            "Validation Accuracy: 0.5022\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.6, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6935\n",
            "Epoch 11 | Avg Loss: 0.6798\n",
            "Epoch 21 | Avg Loss: 0.6777\n",
            "Epoch 31 | Avg Loss: 0.6767\n",
            "Epoch 41 | Avg Loss: 0.6754\n",
            "Validation Accuracy: 0.5225\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.7, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6943\n",
            "Epoch 11 | Avg Loss: 0.6810\n",
            "Epoch 21 | Avg Loss: 0.6786\n",
            "Epoch 31 | Avg Loss: 0.6776\n",
            "Epoch 41 | Avg Loss: 0.6767\n",
            "Validation Accuracy: 0.4693\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.8, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.6954\n",
            "Epoch 11 | Avg Loss: 0.6842\n",
            "Epoch 21 | Avg Loss: 0.6795\n",
            "Epoch 31 | Avg Loss: 0.6783\n",
            "Epoch 41 | Avg Loss: 0.6774\n",
            "Validation Accuracy: 0.5382\n",
            "\n",
            "Trying: lr=0.0001, layers=[128, 64], dropout=0.9, batch_size=32\n",
            "Epoch 1 | Avg Loss: 0.7006\n",
            "Epoch 11 | Avg Loss: 0.6852\n",
            "Epoch 21 | Avg Loss: 0.6823\n",
            "Epoch 31 | Avg Loss: 0.6808\n",
            "Epoch 41 | Avg Loss: 0.6801\n",
            "Validation Accuracy: 0.4458\n",
            "\n",
            "=== Hyperparameter Tuning Results ===\n",
            "1. Accuracy: 0.5382 - LR: 0.0001, Layers: [128, 64], Dropout: 0.8, Batch Size: 32\n",
            "2. Accuracy: 0.5225 - LR: 0.0001, Layers: [128, 64], Dropout: 0.6, Batch Size: 32\n",
            "3. Accuracy: 0.5072 - LR: 0.0001, Layers: [128, 64], Dropout: 0.3, Batch Size: 32\n",
            "4. Accuracy: 0.5045 - LR: 0.0001, Layers: [128, 64], Dropout: 0.2, Batch Size: 32\n",
            "5. Accuracy: 0.5025 - LR: 0.0001, Layers: [128, 64], Dropout: 0.4, Batch Size: 32\n",
            "6. Accuracy: 0.5022 - LR: 0.0001, Layers: [128, 64], Dropout: 0.5, Batch Size: 32\n",
            "7. Accuracy: 0.4975 - LR: 0.0001, Layers: [128, 64], Dropout: 0.0, Batch Size: 32\n",
            "8. Accuracy: 0.4693 - LR: 0.0001, Layers: [128, 64], Dropout: 0.7, Batch Size: 32\n",
            "9. Accuracy: 0.4458 - LR: 0.0001, Layers: [128, 64], Dropout: 0.9, Batch Size: 32\n",
            "\n",
            "Best Configuration: {'learning_rate': 0.0001, 'hidden_layers': [128, 64], 'dropout_rate': 0.8, 'batch_size': 32, 'val_accuracy': 0.538160502910614}\n",
            "\n",
            "=== Training Final Model with Best Hyperparameters ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with mini-batches\n",
        "num_epochs = 100\n",
        "batch_size = best_config['batch_size']\n",
        "num_batches = max(1, len(X_train) // batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Shuffle data\n",
        "    perm = random.permutation(key, len(X_train))\n",
        "    key = random.fold_in(key, epoch)\n",
        "\n",
        "    # Mini-batch updates\n",
        "    total_loss = 0.0\n",
        "    for i in range(num_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        X_batch = X_train[batch_idx]\n",
        "        y_batch = y_train[batch_idx]\n",
        "\n",
        "        params, opt_state, loss = train_step(\n",
        "            params, X_batch, y_batch, opt_state,\n",
        "            dropout_rate=best_config['dropout_rate'], train=True\n",
        "        )\n",
        "        total_loss += loss\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
        "        train_acc = evaluate(params, X_train, y_train, dropout_rate=0.0, train=False)\n",
        "        val_acc = evaluate(params, X_valid, y_valid, dropout_rate=0.0, train=False)\n",
        "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBoWRfJV4KPI",
        "outputId": "8ca87245-c1aa-4420-f3c4-4c20655a5703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.6973 | Train Acc: 0.5278 | Val Acc: 0.4669\n",
            "Epoch 11 | Loss: 0.6830 | Train Acc: 0.5286 | Val Acc: 0.4908\n",
            "Epoch 21 | Loss: 0.6801 | Train Acc: 0.5248 | Val Acc: 0.5311\n",
            "Epoch 31 | Loss: 0.6792 | Train Acc: 0.5377 | Val Acc: 0.5119\n",
            "Epoch 41 | Loss: 0.6784 | Train Acc: 0.5381 | Val Acc: 0.5292\n",
            "Epoch 51 | Loss: 0.6776 | Train Acc: 0.5284 | Val Acc: 0.5432\n",
            "Epoch 61 | Loss: 0.6767 | Train Acc: 0.5268 | Val Acc: 0.5495\n",
            "Epoch 71 | Loss: 0.6763 | Train Acc: 0.5474 | Val Acc: 0.5374\n",
            "Epoch 81 | Loss: 0.6756 | Train Acc: 0.5254 | Val Acc: 0.5487\n",
            "Epoch 91 | Loss: 0.6751 | Train Acc: 0.5418 | Val Acc: 0.5374\n",
            "Epoch 100 | Loss: 0.6749 | Train Acc: 0.5270 | Val Acc: 0.5499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation\n",
        "final_accuracy = evaluate(params, X_valid, y_valid, dropout_rate=0.0, train=False)\n",
        "print(f\"\\nFinal Validation Accuracy: {final_accuracy:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcQjGyOfTKK5",
        "outputId": "2b460511-013d-4441-ee39-2840d3857fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Validation Accuracy: 0.5499022007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Bagging"
      ],
      "metadata": {
        "id": "KpWuQHFdc8Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = acl_train_embedded[:, 0].reshape(-1, 1)\n",
        "X_train = acl_train_embedded[:, 1:]\n",
        "y_valid = acl_valid_embedded[:, 0].reshape(-1, 1)\n",
        "X_valid = acl_valid_embedded[:, 1:]"
      ],
      "metadata": {
        "id": "FQbCbI3anwx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "\n",
        "# List of classifiers to use\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    SVC(kernel='linear'),\n",
        "    RandomForestClassifier(n_estimators=100),\n",
        "    KNeighborsClassifier(n_neighbors=5)\n",
        "]"
      ],
      "metadata": {
        "id": "xKIGgpLiT88g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of bootstrap samples\n",
        "n_bootstrap = 10\n",
        "\n",
        "# Create an empty list to store models\n",
        "trained_classifiers = []\n",
        "\n",
        "# Generate bootstrap samples and train each model\n",
        "for clf in classifiers:\n",
        "    clf_bootstrap_models = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Generate bootstrap sample (with replacement)\n",
        "        X_resampled, y_resampled = resample(X_train, y_train, n_samples=X_train.shape[0], random_state=1000)\n",
        "        clf_clone = clf.__class__()  # Create a fresh clone of the classifier\n",
        "        clf_clone.fit(X_resampled, y_resampled)\n",
        "        clf_bootstrap_models.append(clf_clone)\n",
        "\n",
        "    # Append the trained bootstrap models to the list of classifiers\n",
        "    trained_classifiers.append(clf_bootstrap_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V9BjbUBOc-VZ",
        "outputId": "83235d80-afa7-4421-8f9b-ace28533156f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to aggregate predictions using majority voting\n",
        "def bagging_predict(X):\n",
        "    predictions = []\n",
        "\n",
        "    for clf_bootstrap_models in trained_classifiers:\n",
        "        clf_preds = np.zeros((X.shape[0], len(clf_bootstrap_models)))\n",
        "\n",
        "        for idx, model in enumerate(clf_bootstrap_models):\n",
        "            clf_preds[:, idx] = model.predict(X)\n",
        "\n",
        "        # Average predictions for each classifier group\n",
        "        avg_pred = np.mean(clf_preds, axis=1)\n",
        "        # Convert to binary predictions using threshold\n",
        "        binary_pred = (avg_pred >= 0.5).astype(int)\n",
        "        predictions.append(binary_pred)\n",
        "\n",
        "    # Final ensemble prediction\n",
        "    final_pred = np.mean(predictions, axis=0)\n",
        "    return (final_pred >= 0.5).astype(int)\n",
        "\n",
        "# Apply bagging prediction\n",
        "final_predictions = bagging_predict(X_valid)\n",
        "\n",
        "# Evaluate final predictions (accuracy)\n",
        "accuracy = np.mean(final_predictions == y_valid)\n",
        "print(f\"Bagging Accuracy: {accuracy:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKY33t9adCe5",
        "outputId": "85d3cd1d-acf5-49b5-cc0a-bea312ee87e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.5353250026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Fine-tuned LLM *FinBert* with DoRA"
      ],
      "metadata": {
        "id": "7_q_QB313ORr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets scikit-learn peft numpy torch\n",
        "!pip install accelerate==0.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RJxU9FUf8jcw",
        "outputId": "82ff355f-e5ac-420b-f963-45d34b016d06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.37.2)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.17.0\n",
            "    Uninstalling accelerate-0.17.0:\n",
            "      Successfully uninstalled accelerate-0.17.0\n",
            "Successfully installed accelerate-1.6.0\n",
            "Collecting accelerate==0.17.0\n",
            "  Using cached accelerate-0.17.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.17.0) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.17.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.17.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.17.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.17.0) (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->accelerate==0.17.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->accelerate==0.17.0) (3.0.2)\n",
            "Using cached accelerate-0.17.0-py3-none-any.whl (212 kB)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.6.0\n",
            "    Uninstalling accelerate-1.6.0:\n",
            "      Successfully uninstalled accelerate-1.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.10.0 requires accelerate>=0.21.0, but you have accelerate 0.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2\n",
        "!pip install peft==0.10.0\n",
        "# version debugging referring to https://stackoverflow.com/questions/79273647/cannot-import-name-encoderdecodercache-from-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RCNFttDn_RrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "L39JyTzV86s3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained 'ProsusAI/finbert' model and tokenizer\n",
        "model_name = 'ProsusAI/finbert'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained(model_name)  # Binary classification\n",
        "finbert.config.num_labels = 2\n",
        "finbert.num_labels = 2\n",
        "\n",
        "original_finbert = finbert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTbohJvY4oDb",
        "outputId": "03b7cc95-32c2-4c36-822e-ba63ea4d71e2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add dropout to the classifier for better regularization\n",
        "\n",
        "# First apply the base model modifications\n",
        "dropout_prob = 0.5\n",
        "finbert.classifier = nn.Sequential(\n",
        "    nn.Dropout(dropout_prob),\n",
        "    nn.Linear(finbert.config.hidden_size, finbert.config.num_labels)\n",
        ")\n",
        "\n",
        "# Configure PEFT with LoRA specifically for binary classification\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.5,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"output.dense\"],\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "finbert = get_peft_model(finbert, peft_config)\n",
        "\n",
        "# Manually ensure the classifier parameters are trainable\n",
        "for param in finbert.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "finbert.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYaATiVp60Kj",
        "outputId": "97b6540a-4d38-42a4-95f9-cdf2e62ecd76"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 963,078 || all params: 110,446,856 || trainable%: 0.8719831735183118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup data loader"
      ],
      "metadata": {
        "id": "xqxrF-0ZHzvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "acl_train_ds = Dataset.from_pandas(acl_train_df)\n",
        "acl_valid_ds = Dataset.from_pandas(acl_valid_df)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        padding=True,            # Padding handled dynamically by the DataCollator\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "acl_train_token = acl_train_ds.map(tokenize_function, batched=True)\n",
        "acl_valid_token = acl_valid_ds.map(tokenize_function, batched=True)\n",
        "\n",
        "acl_train_token = acl_train_token.rename_column(\"gold\", \"label\")\n",
        "acl_valid_token = acl_valid_token.rename_column(\"gold\", \"label\")\n",
        "\n",
        "acl_train_token.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "acl_valid_token.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7089b945d50d4c8ab2bbaded97f1ca98",
            "ab0d59b4225d44d1887fe23b69f67912",
            "640381bb940b474e8ee2b9d12e2e2eef",
            "26d5b69b79b14e1fac39cb8562750f49",
            "66fe9607ab8842d0b96a62612cfbe450",
            "eb66125b3e424fabb6b7ec9d67d5bb5f",
            "43d3007c0f11446e87eba71d395c60f9",
            "fbf54e0922de40ed82751ebcb4331771",
            "64213c7a4df64c2ca664ef00263a4b8a",
            "2faf066ca3d74348b89d496efb108934",
            "ec9b5c60740b4dd3b16db0bc9430f5f8",
            "d8ae16d489394b07be41d56e96646370",
            "f1364bda5a8c40ff9c48ff1c65b6856a",
            "8308fa4d767a4042a434ac547f1ee51b",
            "2a28d70f611747278b045d4aba77261d",
            "0b300108d9da4ea8b73d16ba28201621",
            "27821ffe452f461ba2abfde6fa322254",
            "7b33d2ab9fe4489c9167df50a9c246bc",
            "0704f1163773466daba7f077fc004fd4",
            "6ccdcea722404c7f91759173d39e2055",
            "5d3b88db45464c599511c9a13e7007ea",
            "835dd1d7860943b1b681039d18073cff"
          ]
        },
        "id": "KeK4WgPkH1dx",
        "outputId": "dfdd360d-16dd-4a60-cd62-fbeaef0b9d03"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20781 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7089b945d50d4c8ab2bbaded97f1ca98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2555 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8ae16d489394b07be41d56e96646370"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "train_dataloader = DataLoader(acl_train_token, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
        "valid_dataloader = DataLoader(acl_valid_token, batch_size=16, shuffle=False, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "q6qD8Mu3Giz9"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with torch"
      ],
      "metadata": {
        "id": "ura0tssPGfo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "\n",
        "# setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "finbert.to(device)\n",
        "print(device)\n",
        "\n",
        "# set up optimizer\n",
        "optimizer = AdamW(finbert.parameters(), lr=2e-5, weight_decay=0.1)\n",
        "\n",
        "# set up loss function\n",
        "class_weights = torch.tensor([1.0, 1.0], device=device)  # adjust per class stats\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Add early stopping mechanism\n",
        "best_val_accuracy = 0\n",
        "patience = 2\n",
        "early_stop_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "# Add learning rate scheduler for better convergence\n",
        "num_epochs = 5  # Increase epochs since we have early stopping\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"cosine\",  # Use cosine schedule for better convergence\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(0.1 * len(train_dataloader) * num_epochs),  # 10% warmup\n",
        "    num_training_steps=len(train_dataloader) * num_epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BGuX32eGjRA",
        "outputId": "898f07a1-c197-49eb-d450-242c10f2a42d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch training loop with regularization techniques\n",
        "for epoch in range(num_epochs):\n",
        "    finbert.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Enable dropout during training (it's enabled by default in train mode)\n",
        "        outputs = finbert(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Add L2 regularization if needed\n",
        "        # for param in finbert.parameters():\n",
        "        #    loss += 0.01 * torch.sum(param ** 2)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(finbert.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Avg training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    finbert.eval()\n",
        "    correct, total = 0, 0\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = finbert(**batch)\n",
        "            val_loss += outputs.loss.item()\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "            total += batch[\"labels\"].size(0)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    avg_val_loss = val_loss / len(valid_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stop_counter = 0\n",
        "        # Save best model state\n",
        "        best_model_state = {k: v.cpu().clone() for k, v in finbert.state_dict().items()}\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "\n",
        "    if early_stop_counter >= patience:\n",
        "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSD0569AGu7l",
        "outputId": "e15b2ec6-cbd6-40bd-f192-aea338879653"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1299/1299 [03:10<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Avg training loss: 0.7119\n",
            "Validation Loss: 0.6930, Accuracy: 0.5182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1299/1299 [03:10<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Avg training loss: 0.6973\n",
            "Validation Loss: 0.6953, Accuracy: 0.4869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1299/1299 [03:10<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Avg training loss: 0.6887\n",
            "Validation Loss: 0.6970, Accuracy: 0.4814\n",
            "Early stopping triggered after 3 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model state before saving\n",
        "if best_model_state is not None:\n",
        "    finbert.load_state_dict(best_model_state)\n",
        "    print(f\"Loaded best model with validation accuracy: {best_val_accuracy:.10f}\")\n",
        "\n",
        "# save the fine-tuned model\n",
        "finbert.save_pretrained(\"finbert-lora-semantic\")\n",
        "tokenizer.save_pretrained(\"finbert-lora-semantic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1bzew40G9da",
        "outputId": "631a5403-f11a-43e1-b8c3-4b86d3e3c55e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model with validation accuracy: 0.5181996086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('finbert-lora-semantic/tokenizer_config.json',\n",
              " 'finbert-lora-semantic/special_tokens_map.json',\n",
              " 'finbert-lora-semantic/vocab.txt',\n",
              " 'finbert-lora-semantic/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the original FinBERT (before LoRA fine-tuning)\n",
        "\n",
        "# Evaluation function for models\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            val_loss += outputs.loss.item()\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "            total += batch[\"labels\"].size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    avg_loss = val_loss / len(dataloader)\n",
        "    return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "VVGyrAyTSCWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate original FinBERT\n",
        "print(\"\\n=== Evaluating Original FinBERT Model ===\")\n",
        "original_finbert.to(device)\n",
        "original_accuracy, original_loss = evaluate_model(original_finbert, valid_dataloader, device)\n",
        "print(f\"Original FinBERT - Validation Loss: {original_loss:.4f}, Accuracy: {original_accuracy:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6104ed9SD8X",
        "outputId": "8b9e1c62-f6cd-4c1a-a0cf-ac8e2e7eb06d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating Original FinBERT Model ===\n",
            "Original FinBERT - Validation Loss: 0.6930, Accuracy: 0.5181996086\n"
          ]
        }
      ]
    }
  ]
}