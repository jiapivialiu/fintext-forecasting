{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DA2tTnlQjZy"
      },
      "source": [
        "# Neural Nets on Financial Time Seris Forecasting on Text with JAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RQREEVdWQzco",
        "outputId": "a98d995e-1874-4e95-cb7f-fb972f35761c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xnvx_ylwQaSx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "bigdata_train = load_dataset(\"TheFinAI/flare-sm-bigdata\", split=\"train\")\n",
        "bigdata_valid = load_dataset(\"TheFinAI/flare-sm-bigdata\", split=\"validation\")\n",
        "bigdata_test = load_dataset(\"TheFinAI/flare-sm-bigdata\", split=\"test\")\n",
        "\n",
        "bigdata_train_df = bigdata_train.to_pandas()[['gold', 'text']] # 0: rise, 1: fall\n",
        "bigdata_valid_df = bigdata_valid.to_pandas()[['gold', 'text']]\n",
        "bigdata_test_df = bigdata_test.to_pandas()[['gold', 'text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NoEM5P4lQwxo"
      },
      "outputs": [],
      "source": [
        "# embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize the model (using a compact and efficient model)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to get sentence embeddings\n",
        "def get_sbert_embeddings(texts):\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "    return embeddings\n",
        "\n",
        "# Combine embedded text features with non-text features\n",
        "import numpy as np\n",
        "texts = bigdata_train_df['text'].tolist()\n",
        "X_text_embeddings = get_sbert_embeddings(texts)\n",
        "X_text_embeddings = np.array(X_text_embeddings, dtype=np.float32)\n",
        "\n",
        "bigdata_train_embedded = np.concatenate([bigdata_train_df[['gold']], X_text_embeddings], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TZQpjnsvSNwd"
      },
      "outputs": [],
      "source": [
        "texts = bigdata_valid_df['text'].tolist()\n",
        "X_emb = get_sbert_embeddings(texts)\n",
        "X_emb = np.array(X_emb, dtype=np.float32)\n",
        "bigdata_valid_embedded = np.concatenate([bigdata_valid_df[['gold']], X_emb], axis=1)\n",
        "\n",
        "texts = bigdata_test_df['text'].tolist()\n",
        "X_emb = get_sbert_embeddings(texts)\n",
        "X_emb = np.array(X_emb, dtype=np.float32)\n",
        "bigdata_test_embedded = np.concatenate([bigdata_test_df[['gold']], X_emb], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85cP-6q-Q9x2",
        "outputId": "612dec8a-b382-47cd-970e-3a1bb7eb46ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4897, 385)\n"
          ]
        }
      ],
      "source": [
        "# Convert the combined data to JAX arrays if needed\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, random\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from jax import random\n",
        "\n",
        "bigdata_train_embedded_jax = jnp.array(bigdata_train_embedded, dtype=jnp.float32)\n",
        "print(bigdata_train_embedded_jax.shape)\n",
        "\n",
        "bigdata_test_embedded_jax = jnp.array(bigdata_test_embedded, dtype=jnp.float32)\n",
        "bigdata_valid_embedded_jax = jnp.array(bigdata_valid_embedded, dtype=jnp.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LSogRwgPR2ZB"
      },
      "outputs": [],
      "source": [
        "# initialize random parameters\n",
        "def init_params(layer_sizes, key):\n",
        "    \"\"\"Initialize parameters for a simple MLP model.\"\"\"\n",
        "    params = []\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        # Initialize weights with a small random value and biases as zeros\n",
        "        w_key, b_key = random.split(key)\n",
        "        w = random.normal(w_key, (layer_sizes[i], layer_sizes[i+1])) * jnp.sqrt(2.0 / layer_sizes[i])\n",
        "        b = jnp.zeros((layer_sizes[i+1],))\n",
        "        params.append((w, b))\n",
        "    return params\n",
        "\n",
        "# Define the neural nets with enhancements\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU activation function.\"\"\"\n",
        "    return jnp.where(x > 0, x, alpha * x)\n",
        "\n",
        "def mlp(params, X, dropout_key=None, dropout_rate=0.2):\n",
        "    \"\"\"A feedforward MLP with dropout.\"\"\"\n",
        "    for i, (w, b) in enumerate(params[:-1]):\n",
        "        X = jnp.dot(X, w) + b\n",
        "        X = leaky_relu(X)  # Use LeakyReLU activation\n",
        "        if dropout_key is not None:\n",
        "            # Apply dropout during training\n",
        "            dropout_key, subkey = random.split(dropout_key)\n",
        "            mask = random.bernoulli(subkey, p=1 - dropout_rate, shape=X.shape)\n",
        "            X = X * mask / (1 - dropout_rate)\n",
        "    w, b = params[-1]\n",
        "    return jnp.dot(X, w) + b  # Linear output layer (regression)\n",
        "\n",
        "# Loss function to binary cross-entropy\n",
        "def binary_cross_entropy_loss(params, X, y):\n",
        "    \"\"\"Compute the binary cross-entropy loss.\"\"\"\n",
        "    logits = mlp(params, X)\n",
        "    preds = jax.nn.sigmoid(logits)  # Apply sigmoid for binary classification\n",
        "    return -jnp.mean(y * jnp.log(preds + 1e-8) + (1 - y) * jnp.log(1 - preds + 1e-8))\n",
        "\n",
        "# compute gradients\n",
        "grad_loss_fn = grad(binary_cross_entropy_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_nwzp-JR4iB",
        "outputId": "8c40c9a6-eef7-42a1-826d-92e774bd80e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.01446111 -0.03051838  0.00084274 ... -0.08114446 -0.029073\n",
            "   0.0523665 ]\n",
            " [ 0.03013896 -0.00629956  0.00309164 ... -0.09023841 -0.05489946\n",
            "   0.03478851]\n",
            " [ 0.01602308 -0.03069994 -0.00132307 ... -0.07940799 -0.03194\n",
            "   0.04441951]]\n",
            "(4897, 384)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "(4897,)\n"
          ]
        }
      ],
      "source": [
        "# define a training step\n",
        "@jit\n",
        "def train_step(params, X, y, key, learning_rate=0.001, dropout_rate=0.2):\n",
        "    \"\"\"Perform one step of gradient descent with dropout.\"\"\"\n",
        "    dropout_key, subkey = random.split(key)\n",
        "    grads = grad_loss_fn(params, X, y)\n",
        "    new_params = [(w - learning_rate * dw, b - learning_rate * db)\n",
        "                  for (w, b), (dw, db) in zip(params, grads)]\n",
        "    return new_params, dropout_key\n",
        "\n",
        "# Training data (replace with your own)\n",
        "X_train = bigdata_train_embedded_jax[:, 1:]\n",
        "print(X_train[:3,:])\n",
        "print(X_train.shape)\n",
        "y_train = bigdata_train_embedded_jax[:, 0]\n",
        "print(y_train[:10])\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid = bigdata_valid_embedded_jax[:, 1:]\n",
        "y_valid = bigdata_valid_embedded_jax[:, 0]\n",
        "\n",
        "X_test = bigdata_test_embedded_jax[:, 1:]\n",
        "y_test = bigdata_test_embedded_jax[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHNYQxYgR7o-",
        "outputId": "fa8dc6b1-021f-4c65-b774-4e5cb2f8236e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.6922121644020081\n",
            "Epoch 100, Loss: 0.6917664408683777\n",
            "Epoch 200, Loss: 0.6916472911834717\n",
            "Epoch 300, Loss: 0.6916123032569885\n",
            "Epoch 400, Loss: 0.6916014552116394\n",
            "Epoch 500, Loss: 0.6915980577468872\n",
            "Epoch 600, Loss: 0.6915969848632812\n",
            "Epoch 700, Loss: 0.6915965676307678\n",
            "Epoch 800, Loss: 0.6915964484214783\n",
            "Epoch 900, Loss: 0.6915963888168335\n",
            "Epoch 999, Loss: 0.6915963888168335\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "# Initialize model parameters\n",
        "key = random.PRNGKey(0)\n",
        "layer_sizes = [X_train.shape[1], 128, 64, 32, 1]\n",
        "params = init_params(layer_sizes, key)\n",
        "dropout_key = random.PRNGKey(1)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    params, dropout_key = train_step(params, X_train, y_train, dropout_key, learning_rate=0.005, dropout_rate=0.2)\n",
        "    if epoch % 100 == 0 or epoch == num_epochs - 1:\n",
        "        loss = binary_cross_entropy_loss(params, X_train, y_train)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhjWP-5cR9aR",
        "outputId": "361ac373-e359-41d0-d8d9-c134a5e84c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6958872676\n",
            "Validation Accuracy: 0.4887217879\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set with sigmoid activation\n",
        "valid_logits = mlp(params, X_valid)\n",
        "valid_preds = jax.nn.sigmoid(valid_logits)\n",
        "valid_loss = binary_cross_entropy_loss(params, X_valid, y_valid)\n",
        "print(f\"Validation Loss: {valid_loss:.10f}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "valid_preds_binary = (valid_preds > 0.5).astype(jnp.float32)\n",
        "accuracy = jnp.mean(valid_preds_binary == y_valid)\n",
        "print(f\"Validation Accuracy: {accuracy:.10f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV8TVajgTqyH",
        "outputId": "b899ce0b-7467-4ef9-8b46-898e734b114a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       ...,\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict on test set\n",
        "test_preds = mlp(params, X_test)\n",
        "test_preds_binary = (test_preds > 0.5).astype(jnp.float32)\n",
        "\n",
        "test_preds_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-KgYPpeVAWr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
